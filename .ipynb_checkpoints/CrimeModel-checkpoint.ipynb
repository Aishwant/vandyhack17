{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################################Ethan L. Mines##############################\n",
    "#############################################VandyHacks IV: October 20-22###############\n",
    "import numpy as np\n",
    "import cntk as C\n",
    "\n",
    "####################################Hyperparameters#####################################\n",
    "numType = np.float32\n",
    "numLayers = 1\n",
    "learning_rate = 0.1\n",
    "training_ratio = 3 / 4\n",
    "learnerFunction = C.sgd\n",
    "mb_size = 25\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_arrays(*arrays):\n",
    "    rand_state = np.random.get_state()\n",
    "    for array in arrays:\n",
    "        np.random.set_state(rand_state)\n",
    "        np.random.shuffle(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifier(output_dim, input_var, num_layers):\n",
    "    z = Dense(output_dim)(input_var) #FIXME: Change initial variable name\n",
    "    for i in range(num_layers - 1):\n",
    "        z = Dense(output_dim)(z)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_trainer(z, labels):    \n",
    "    lr_schedule = C.learning_rate_schedule(learning_rate, C.UnitType.minibatch)\n",
    "    learner = learnerFunction(z.parameters, lr_schedule)\n",
    "    loss = C.cross_entropy_with_softmax(z, labels)\n",
    "    eval_error = c.classification_error(z, labels)\n",
    "    trainer = C.Trainer(z, (loss, eval_error), [learner])\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################Modified from Microsoft CNTK101 Tutorial#####################\n",
    "def print_progress(trainer, mb_num, frequency, display = True):\n",
    "    loss = \"NA\"\n",
    "    if mb_num % frequency == 0:\n",
    "        loss = trainer.previous_minibatch_loss_average\n",
    "        eval_error = trainer.previous_minibatch_evaluation_average\n",
    "        if display:\n",
    "            print(\"Minibatch {0:04n}: Average Loss = {1:.4f}, Average Error = {2:.4f}\".format(mb_num, loss, eval_error))\n",
    "    return mb_num, loss, eval_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################Modified from Microsoft CNTK101 Tutorial#################\n",
    "from collections import defaultdict\n",
    "\n",
    "def train_model(trainer, training_size, mb_size, features, labels, input_var, label_var):\n",
    "    training_plot_data = defaultdict(list)\n",
    "    progress_output_freq = 1000\n",
    "    training_features = features[:training_size]\n",
    "    training_labels = labels[:training_size]\n",
    "    \n",
    "    num_training_mbs = training_size // mb_size\n",
    "    \n",
    "    for i in range(num_training_mbs):\n",
    "        start_index = i * training_minibatch_size\n",
    "        end_index = start_index + training_minibatch_size\n",
    "        feature_batch = training_features[start_index : end_index]\n",
    "        label_batch = training_labels[start_index : end_index]\n",
    "        trainer.train_minibatch({input_var : feature_batch, label_var : label_batch})\n",
    "    \n",
    "        mb_num, loss, error = print_progress(trainer, i, progress_output_freq)\n",
    "        if loss != \"NA\":\n",
    "            training_plotdata[\"Batch Number\"].append(mb_num)\n",
    "            training_plotdata[\"Loss\"].append(loss)\n",
    "            training_plotdata[\"Error\"].append(error)\n",
    "            \n",
    "    return training_plotdata\n",
    "\n",
    "def test_model(trainer, training_size, mb_size, features, labels, input_var, label_var):\n",
    "    testing_plotdata = defaultdict(list)\n",
    "    testing_features = features[training_size:]\n",
    "    testing_labels = labels[training_size:]\n",
    "    \n",
    "    num_testing_mbs = len(features) - training_size\n",
    "    for i in range(num_testing_mbs):\n",
    "        start_index = i * mb_size\n",
    "        end_index = start_index + mb_size\n",
    "        feature_batch = testing_features[start_index : end_index]\n",
    "        label_batch = testing_labels[start_index : end_index]\n",
    "    \n",
    "        eval_error = trainer.test_minibatch({input_var : feature_batch, label_var : label_batch})\n",
    "    \n",
    "        if eval_error != \"NA\":\n",
    "            testing_plotdata[\"Batch Number\"].append(i)\n",
    "            testing_plotdata[\"Error\"].append(eval_error)\n",
    "            \n",
    "    return testing_plotdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def graphCriteria(training_plotdata, testing_plotdata):\n",
    "\n",
    "    training_plotdata[\"Average Loss\"] = moving_average(training_plotdata[\"Loss\"])\n",
    "    training_plotdata[\"Average Error\"] = moving_average(training_plotdata[\"Error\"])\n",
    "\n",
    "    testing_plotdata[\"Average Error\"] = moving_average(testing_plotdata[\"Error\"])\n",
    "\n",
    "    plt.plot(training_plotdata[\"Batch Number\"], training_plotdata[\"Average Loss\"])\n",
    "    plt.title(\"Cumulative Average Loss\")\n",
    "    plt.xlabel(\"Minibatch Number\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Average Training Loss = {0:.2f}\".format(training_plotdata[\"Average Loss\"][-1]))\n",
    "\n",
    "\n",
    "    plt.hist(testing_plotdata[\"Loss\"])\n",
    "    plt.title(\"Testing Error\")\n",
    "    plt.xlabel(\"Minibatch Average Error\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Average Testing Loss = {0:.2f}\".format(testing_plotdata[\"Average Loss\"][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def moving_average(loss_list):\n",
    "    averages = np.cumsum(np.asarray(loss_list))\n",
    "    for i in range(len(averages)):\n",
    "        averages[i] = averages[i] / (i + 1)\n",
    "    return averages\n",
    "\n",
    "#Expects number of possible categories \"num_categories\" and category index \"category\" in range 0...num_categories-1\n",
    "def toOneHot(column, indexDict):\n",
    "    oneHot = np.zeros( (len(column), len(indexDict)), dtype = numType)\n",
    "    \n",
    "    for i in range(len(column))\n",
    "        oneHot[i][ indexDict[column[i]] ] = 1\n",
    "    \n",
    "    return oneHot\n",
    "\n",
    "#############Returns str->int dictionary where the int's are indices for one-hot vectors of categories in column\n",
    "def extractCategories(column, categPath):\n",
    "    dictionary = {}\n",
    "    i = 0\n",
    "    with open(categPath, \"w\") as categDict:\n",
    "        for element in column:\n",
    "            if element not in dictionary:\n",
    "                dictionary[element] = i\n",
    "                i += 1\n",
    "                categDict.write(element + \"\\n\")\n",
    "    return dictionary\n",
    "\n",
    "#One-hot encodes all the CSV data\n",
    "#Also writes simple text files that can than be used to reconstruct the index dictionaries\n",
    "def process_csv(source, model_prefix):\n",
    "    data = np.genfromtxt(source, delimiter = \",\", dtype = str)\n",
    "    dictDir = \"dicts/\"\n",
    "    \n",
    "    #print(data[:5, 1])\n",
    "    \n",
    "    column_indices = {\"month\": 1,\n",
    "                     \"hour\": 2,\n",
    "                     \"weekday\": 3,\n",
    "                     \"zip_code\": 4,\n",
    "                     \"crime\": 7\n",
    "                     }\n",
    "    \n",
    "    months = data[:, column_indices[\"month\"]]\n",
    "    hours = data[:, column_indices[\"hour\"]]\n",
    "    weekdays = data[:, column_indices[\"weekday\"]]\n",
    "    zips = data[:, column_indices[\"zip_code\"]]\n",
    "    crimes = data[:, column_indices[\"crime\"]]\n",
    "    \n",
    "    monthsDict = extractCategories(months, dictDir + prefix + \"_month_dict\")\n",
    "    hoursDict = extractCategories(hours, dictDir + prefix + \"_hours_dict\")\n",
    "    weekdaysDict = extractCategories(weekdays, dictDir + prefix + \"_weekdays_dict\")\n",
    "    zipsDict = extractCategories(zips, dictDir + prefix + \"_zips_dict\")\n",
    "    crimesDict = extractCategories(crimes, dictDir + prefix + \"_crimes_dict\")\n",
    "    \n",
    "    months = toOneHot(months, monthsDict)\n",
    "    hours = toOneHot(hours, hoursDict)\n",
    "    weekdays toOneHot(weekdays, weekdaysDict)\n",
    "    zips = toOneHot(zips, zipsDict)\n",
    "    crimes = toOneHot(crimes, crimesDict)\n",
    "    \n",
    "    shuffle_arrays(months, hours, weekdays, zips, crimes)\n",
    "    features = np.column_stack(months, hours, weekdays)\n",
    "    \n",
    "    return features, zips, crimes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5' '5' '5' '5' '5']\n",
      "['5' '5' '5' '5' '5' '5' '5' '5' '5' '5' '5' '5' '5' '5' '5' '6' '6' '6'\n",
      " '6' '6']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def gen_models(source, model_prefix):\n",
    "\n",
    "    features, zips, crimes = process_csv(source, model_prefix)\n",
    "    \n",
    "    #Model variables\n",
    "    input = C.input_variable(num_features, dtype = numType)    \n",
    "    zip_label = C.input_variable(num_zips, dtype = numType)\n",
    "    crime_label = C.input_variable(num_crimes, dtype = numType)\n",
    "    \n",
    "    #Actual models\n",
    "    zone_classifier = classifier(zip_label.shape[0], training_data, num_layers)\n",
    "    crime_classifier = classifier(crime_label.shape[0], training_data, num_layers)\n",
    "    \n",
    "    zip_trainer = my_trainer(num_zone_classes, zone_classifier)\n",
    "    crime_trainer = my_trainer(num_crime_classes, crime_classifier)\n",
    "    \n",
    "    num_samples = len(zips)\n",
    "    num_training = int(num_samples * training_ratio)\n",
    "    \n",
    "    train_model(zip_trainer, num_training, mb_size, features, zips, input, zip_label)\n",
    "    train_model(crime_trainer, num_training, mb_size, features, crimes, input, crime_label)\n",
    "    \n",
    "    test_model(zip_trainer, num_training, mb_size, features, zips, input, zip_label)\n",
    "    test_model(crime_trainer, num_training, mb_size, features, crimes, input, crime_label)\n",
    "    \n",
    "    return zone_classifier, crime_classifier\n",
    "    \n",
    "gen_models(\"Dataset/cleanedArizona.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_model_files(source, model_prefix):\n",
    "\n",
    "    zone_model, crime_model = gen_models(source)\n",
    "    zone_model_path = model_prefix + \"_zones.cmf\"\n",
    "    crime_model_path = model_prefix + \"_crimes.cmf\"\n",
    "    \n",
    "    zone_model.save(zone_model_path)\n",
    "    print(\"Wrote \" + zone_model_path)\n",
    "    crime_model.save(crime_model_path)\n",
    "    print(\"Wrote \" + crime_model_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataPath = \"Dataset\"\n",
    "datafiles = [\"cleanedArizona.csv\"]\n",
    "model_prefixes =[\"arizona\"] # For naming saved model files\n",
    "\n",
    "for i in len(datafiles):\n",
    "    write_model_files(dataPath + \"/\" + datafiles[i], model_prefixes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
